---
title: "miniproject"
format: pdf
toc: true
---
Presets: 
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
```

# Import and Prepare Data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names = 1)
head(wisc.df)
```
Get rid of the diagnosis column as it is the "answer" to the question we want to answer with unsupervised learning.
```{r}
wisc.data <- wisc.df[,-1]
```

New diagnosis vector to fill in later: 
```{r}
diagnosis <- as.factor(wisc.df[,1])
# Check it works
head(diagnosis)
```
> Q1. How many observations are in the dataset?

```{r}
nrow(wisc.data)
```
There are `r nrow(wisc.data)` rows (or observations) in this dataset.

> Q2. How many observations have a malignant diagnosis?

```{r}
table(diagnosis)
```
There are 212 observations with a malignant diagnosis.

> Q3. How many variables/features are suffixed with `_mean`?

```{r}
length(grep("_mean",names(wisc.data)))
```
There are `r length(grep("_mean",names(wisc.data)))` variables suffixed with `_mean`.

# PCA

Check if data needs to be scaled
```{r}
colMeans(wisc.data)
```
```{r}
apply(wisc.data,2,sd)
```

Do PCA: 
```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
# Summarize results
summary(wisc.pr)
```
> Q4. From results, what proportion of original variance is captured by PC1?

From `summary(wisc.pr)` we know PC1 accounts for 44.27% of the original variance.

> Q5. How many PCs are required to capture >70% of the variance?

3 PCs (1, 2, 3) are needed, at which point the Cumulative Proportion of variance captured is 72.6%.

> Q6. How many PCs are required to capture >90% of the variance?

The first 7 PCs are needed to capture >90% of the variance.

## Interpreting PCA results

Making a biplot
```{r}
biplot(wisc.pr)
```
> Q7. What stands out about this plot, is it easy to understand?

No the plot is incredibly busy with numbers and variables resulting in overcrowding/overplotting.

Making a scatter plot:

```{r}
plot(wisc.pr$x, col = diagnosis, xlab = "PC1", ylab = "PC2")
```

> Q8. Plot for PC1 and PC3, what do you notice?

```{r}
plot(wisc.pr$x[, c(1, 3)], col = diagnosis, xlab = "PC1", ylab = "PC3")
```
The data overlap more in the PC1/PC3 plot since PC3 explains less variance than PC2. 

Using ggplot

```{r}
# make dataframe
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# preset theme
theme_set(theme_bw())

# make scatterplot
ggplot(df) + 
  aes(PC1, PC2, col = diagnosis) + geom_point()
```
## Variance Explained

Calculate variance (sd squared)
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate for each PC:
```{r}
pve <- pr.var / sum(pr.var)
```

Plot
```{r}
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "o")
```
Alternative scree plot

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
# axis(2, at=pve, labels=round(pve,2)*100 )
```

# Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)
head(data.scaled)
```

Calculate Euclidean distance
```{r}
data.dist <- dist(data.scaled)
```

Create model using complete linkage
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q10. Plot and find the height where the model has 4 clusters

```{r, fig.height = 10}
plot(wisc.hclust) + abline(h = 19, col = "red", lty = 2)
```

## Selecting Number of Clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h = 19)
```

Compare cluster membership to actual diagnoses.
```{r}
table(wisc.hclust.clusters, diagnosis)
```
## Using Different methods
Methods to combine points during hierarchical clustering:
```{r}
wisc.hclust.single <- hclust(data.dist, method = "single")
wisc.hclust.average <- hclust(data.dist, method = "average")
wisc.hclust.ward.D2 <- hclust(data.dist, method = "ward.D2")
```

Single method plot
```{r, fig.height = 10}
plot(wisc.hclust.single) + abline(h = 19, col = "red", lty = 2)
```
Average plot
```{r, fig.height = 10}
plot(wisc.hclust.average) 
```
```{r, fig.height = 10}
plot(wisc.hclust.ward.D2) 
```


> Q12. Which method gives your favorite results from data.dist?

Personally I liked that method ward.D2 because 1) the largest height vertical bars (largest Euclidean distance between sets) are in the first divide. This makes intuitive sense because from the diagnoses results we know there are 2 options, ideally most of the data should be clustered into these two groups. 2) The output tree is the easiest to read since it separates the groups more evenly, instead of separating one observation at a time.

# Combining methods
Clustering on PCA results
```{r}
# We only want the first 7 PCs
head(wisc.pr$x[,1:7])
# Find Euclidean distance
pca.dist <- dist(wisc.pr$x[,1:7])
# do clustering
wisc.pr.hclust <- hclust(pca.dist, method = "ward.D2")
```

Plot
```{r, fig.height = 9}
plot(wisc.pr.hclust)
```
Are the two main clusters the malignant and benign groups?

```{r}
grps <- cutree(wisc.pr.hclust, k = 2)
table(grps)
```
Match to diagnoses:
```{r}
table(grps, diagnosis)
```
Visualize
```{r}
plot(wisc.pr$x[,1:2], col = grps)
```
```{r}
plot(wisc.pr$x[,1:2], col = diagnosis)
```
Turn groups into factor
```{r}
g <- as.factor(grps)
levels(g)
```
Reorder
```{r}
g <- relevel(g, 2)
levels(g)
```
New plot: 
```{r}
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
# Use the distance of first 7 PCs to cluster
wisc.pr.hclust <- hclust(pca.dist, method="ward.D2")
# cluster into 2 groups
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
# compare results from model with known diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
> Q13. What about clustering with 4 groups, do the results separate out the diagnoses well?

```{r}
wisc.pr.hclust.clusters4 <- cutree(wisc.pr.hclust, k = 4)
table(wisc.pr.hclust.clusters4, diagnosis)
```

> Q14. How well do hierarchical clustering models from previous sections (before PCA) do in separating diagnoses?

```{r}
head(wisc.hclust.clusters)
table(wisc.hclust.clusters, diagnosis)
```
In this method there are fewer Malignant diagnoses in their own cluster. PCA clustering worked better.

# Sensitivity/Specificity

> Q15. Which analysis had best sensitivity? Which had best specificity?

# Prediction

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
newdata <- read.csv(url)
npc <- predict(wisc.pr, newdata = newdata)
npc
```

Plotting new cancer data on the old PCA model
```{r}
plot(wisc.pr$x[,1:2], col = g) + points(npc[,1], npc[,2], col = "blue", pch = 16, cex = 3) + text(npc[,1], npc[,2], c(1,2), col = "white")
```
Patient 2 should be prioritized as their data is clearly within the malignant cluster.

```{r}
sessionInfo()
```

